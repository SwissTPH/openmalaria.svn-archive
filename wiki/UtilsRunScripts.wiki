#summary Synopsis of Python scripts to run and analyse test scenarios

= Util scripts =

We have developed a few scripts to ease running and analysing the output of individual or small numbers of scenarios:

  * [UtilsRunScripts#run.py run.py]
  * [UtilsRunScripts#compareOutput.py compareOutput.py]
  * [UtilsRunScripts#compareCtsout.py compareCtsout.py]
  * [UtilsRunScripts#plotResult.py plotResult.py]

The first of these is used by [http://cmake.org/ ctest] to [http://code.google.com/p/openmalaria/wiki/BuildSystem#Running_the_tests run the built-in tests].


= run.py =

Code: [http://code.google.com/p/openmalaria/source/browse/trunk/test/run.py test/run.py]

This script can run one or more scenarios in a temporary directory with various options and test the output is as expected. It is designed for running the built-in test scenarios and aiding further analysis when these do not work as expected, and thus is sometimes more convenient than [UtilsRunning running scenarios directly].

Scenarios to be run must be in the `test` directory and named `scenarioX.xml` for some `X`. Passing run.py the argument `ABC` will then run `scenarioABC.xml` etc. — multiple names may be passed, or none, in which case run.py will attempt to run all XML files of this form in the `test` dir.

The run.py script in the test directory cannot be run directly but is translated by cmake into the build directory. Thus, after [BuildSystem building OpenMalaria], from the build directory, run.py can be run as

{{{
# run scenarios 1, 2 and 3.
test/run.py 1 2 3
}}}

Options can be passed to OpenMalaria by first specifying -- (which marks the end of options passed to the script); examples:

{{{
test/run.py VecTest -- --print-EIR
test/run.py 1 -- --checkpoint
}}}

In the last case, openMalaria exits after each checkpoint is written, and run.py starts openMalaria again (until either output.txt is written or no new checkpoints appear to have been written).

Various options can also be given to run.py:

{{{
  -h, --help            show this help message and exit
  -q, --quiet           Turn off console output from this script
  -n, --dry-run         Don't actually run openMalaria, just output the
                        commandline.
  -c, --dont-cleanup    Don't clean up expected files from the temparary dir
                        (checkpoint files, schema, etc.)
  -d, --diff            Launch a diff program (kdiff3) on the output if
                        validation fails
  --valid, --validate   Validate the XML file(s) using xmllint and the latest
                        schema.
  -g, --gdb             Run openMalaria through gdb.
  --valgrind            Run openMalaria through valgrind (using memcheck
                        tool).
  --valgrind-track-origins
                        As --valgrind, but pass --track-origins=yes option
                        (1/2 performance).
  --callgrind           Run openMalaria through valgrind using callgrind tool.
  --cachegrind          Run openMalaria through valgrind using cachegrind
                        tool.
}}}

As an example, `test/run.py Cohort -c` runs the Cohort scenario, but doesn't delete expected temporary files from the output directory so you can inspect what's there (or run openMalaria directly from there). Note that any unexpected files and output files which are different from expectation are not deleted in any case.

After running scenarios, the generated output ([http://code.google.com/p/openmalaria/wiki/OutputFiles#Surveys output.txt] and [http://code.google.com/p/openmalaria/wiki/OutputFiles#Continuous_monitoring ctsout.txt]) is compared against that in [http://code.google.com/p/openmalaria/source/browse/#svn%2Ftrunk%2Ftest%2Fexpected `OMROOT/test/expected`] using the compareOutput.py and compareCtsout.py scripts in `OMROOT/util`. See below for how to interpret their output.

== compareOutput.py ==

Code: [http://code.google.com/p/openmalaria/source/browse/trunk/util/compareOutput.py util/compareOutput.py]

This script is designed to test whether two output.txt files are identical, similar or completely different. It is used in testing, usually through `run.py`.

When the output files are not identical, the script parses the output files (can take a while when they are large). It may be that although the output files are not byte-for-byte identical, the output is never-the-less exactly or to a high degree of precision equivalent. In these cases not much output is printed.

When significant differences are discovered, a line is printed for each measure with significant differences, as well as a summary. The per-measure lines look like:

{{{
for measure M:  sum(1st file):X  sum(2nd file):Y  diff/sum: U  (abs diff)/sum:V
}}}

where M is the measure's numeric identifier (see table in SurveyMeasure.h or on
wiki), X and Y are the sums of all values for this measure where a value is
given in both files, U is the sum of signed differences between values from the
first and second file for this measure divided by X, and V is the same accept
that the absolute value of the differences is taken before summing.

U will be positive if Y>X, negative if X>Y, zero if X=Y, assuming X!=0. If X=0
both U and V will be nan. The absolute value of U should never be larger than
V.

If abs(U) is much less than 1 (say under 0.1, or 10%) it indicates X and Y are similar: the total of the measure (across age groups and time points) is similar. If V is also small it gives a stronger indication: that the values at each (age) group and time-point are similar. Of course if measuring low frequency discrete random events and 2 occur in one simulation while none occur in another, it doesn't indicate there's much difference between scenarios. Similarly if 200 random events are reported in the whole simulation but split between 100 per-group, per-time-point records, it shouldn't be surprising if V is large even if U is small and the scenarios are very similar. It is therefore not really possible to conclude that two scenarios are similar based on their outputs unless the population size is quite large.

As summary, this script reports the "total relative diff". This is the some of a difference metric for each pair of values which can never be greater than 1 (assuming no special values like infinity are input) and will be less than the relative precision (typically 10¯⁶<wiki:comment>10^(-6)</wiki:comment>) for each pair considered approximately equal.


== compareCtsout.py ==

Code: [http://code.google.com/p/openmalaria/source/browse/trunk/util/compareCtsout.py util/compareCtsout.py]

This functions similarly to compareOutput.py but reads the ctsout.txt files. It needs [http://matplotlib.sourceforge.net/ matplotlib] installed to work.

= plotResult.py =

Code: [http://code.google.com/p/openmalaria/source/browse/trunk/util/plotResult.py util/plotResult.py]

This is a script designed to produce rapid visualisations of output.txt files. An example (normally labels are added, but on a small plot like this they hide the graphs):

http://openmalaria.googlecode.com/svn/wiki/graphs/plot-scenario-cohort.svg

Options:

{{{
$ util/plotResult.py -h
Usage: plotResult.py [options] FILES

Plots results from an OpenMalaria (surveys) output file by time. Currently no
support for simultaeneously handling multiple files or plotting according to
age group.  Valid targets for plotting keys are: none (key is aggregated),
x-axis, plot, line. If no key is set to the x-axis, the first unassigned of
survey, group, file will be assigned to the x-axis.

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -e FILTEREXPR, --filter=FILTEREXPR
                        Filter entries read according to this rule (i.e.
                        values are included when this returns true).
                        Parameters available: f, m, s, g. Examples: 'True',
                        'm!=0' (default), 'm in [11,12,13]', 's > 73 and
                        m!=0'.
  --debug-filter        Each time FILTEREXPR is called, print input values and
                        output. Warning: will print a lot of data!
  -a, --no-auto-measures
                        Don't automatically put similar measures on the same
                        plot.
  -s S, --survey=S      How to plot surveys
  -g G, --group=G       How to plot (age) groups
  -f F, --file=F        How to plot outputs from different files
  -n, --file-names      Use full file names instead of replacing with short
                        versions
  -l, --no-legends      Turn off legends (sometimes they hide parts of the
                        plots)
}}}